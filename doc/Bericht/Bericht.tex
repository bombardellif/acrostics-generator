%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt]{reportAlternative}

\setlength{\parindent}{0em} 

%\usepackage[german]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx} % Required to insert images
\usepackage{float}
\usepackage{listings}
\usepackage{url}
\usepackage{alltt}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[nottoc]{tocbibind}	
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{caption}
\usepackage{amsmath}

\settocbibname{References}

\graphicspath{ {img/} }
\bibliographystyle{plainnat}


% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1} % Line spacing

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
\includegraphics[scale=0.5]{tub_logo}\\
\Large{
Technische Universität Berlin\\
Fakultät IV - Fakultät Elektrotechnik und Informatik\\
Fachgebiet Datenbanksysteme und Informationsmanagement
}\\
\vspace{1.5cm}\textbf{Project Report\\
Generating Acrostics via Paraphrasing and Heuristic Search\\
DBPRO - Database Projects (WS 2014/2015)
}\\
}

\author{
\vspace{1.5cm}\\
Supervisor: \\
Johannes Kirschnick \\
\\
Authors:\\
Bruno Soares Fillmann () \\
Fernando Bombardelli da Silva (bombardelli.f@mailbox.tu-berlin.de)\\
Jürgen Bauer (jbauer@mailbox.tu-berlin.de) \\
\vspace{2cm}William Bombardelli da Silva (wbombardellis@mailbox.tu-berlin.de)}

\date{February 2$^{nd}$, 2015} % Insert date here if you want it to appear below your name

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

%\setcounter{tocdepth}{1} % Uncomment this line if you don't want subsections listed in the ToC

%\newpage
\tableofcontents
\newpage

%----------------------------------------------------------------------------------------
% Examples of elements for the report in latex
%----------------------------------------------------------------------------------------
%================
%Inserting code:
%\lstinputlisting[frame=single, breaklines=true, language=java, label=lst:Foo, caption=Foo]{foo.java}
%================
%Inserting code inline:
%\begin{lstlisting}[frame=single, breaklines=true, language=java]
%//CODE HERE
%\end{lstlisting}
%================
%Define a label for furutre referenceing: \label{sec:Foo}
%Refecing a label: \ref{sec:Foo}
%================
%Citing Bibliography: \cite{NameOfBibItem}
%================
%Emphasis on text: \emph{foo}
%Bold on text: \textbf{Foo}
%================
%Table Example:
%\begin{table}[h]
%\centering
%\begin{tabular}{l | l | l | l | l}
%	\hline
%	\textbf{Modificador} & \textbf{Classe} & \textbf{Pacote} & \textbf{Subclasse} & \textbf{Mundo} \\ \hline
%		\textbf{\textit{public}} &	S &		S &	S &	S \\ \hline
%		\textbf{\textit{protected}} &	S &		S &	S &	N \\ \hline
%		\textit{sem modificador} & S &	S &	N &	N \\ \hline
%		\textbf{\textit{private}} &	S &		N &	N &	N \\
%	\hline
%\end{tabular}
%\caption{Tabela de Modificadores de Acesso de Java}
%\end{table}
%================
%Example of figure
%\begin{figure}[H]
%\centering
%\includegraphics[scale=0.5]{img_1_1}
%\caption{\label{}Imagem -- Diagrama Conceitual de Java}
%\end{figure}
%================
%Example of acrostic text
%\begin{figure}[H]
%\begin{quote}
%\begin{alltt}
%[MY EXAMPLE]
%\end{alltt}
%\end{quote}
%\caption{Example of synonym application $-$ Paraphrased Text}
%\end{figure}
%
%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
%	Abstract
%----------------------------------------------------------------------------------------
\begin{abstract}
We look here into the creation of acrostics in German from random texts. The approach taken was the paraphrasing of texts using both semantic and orthographic operators to generate new possible texts and find an altered version of the original text that encodes the determined acrostic. With the use of the A* algorithm, we search for the solution to the problem in a tree created from the generated paraphrased texts. We've achieved good results for short words and 
\end{abstract}

%----------------------------------------------------------------------------------------
%	Introduction and Motivation
%----------------------------------------------------------------------------------------
\chapter{Introduction and Motivation}
\section{Objective}
We have with this project the objective of generating acrostics from random texts for the german language. This work is based on the paper on the generation of acrostics by Benno Stein, Matthias Hagen, and Christof Bräutigam \cite{Stein}. The paper proposes an algorithm which works for the creation of acrostics in the english language. Taking the ideas demonstrated in \cite{Stein} as inspiration, here we intend to create an algorithm which adapts the solutions proposed in the paper to German. 

To do this we decided on the use of a search strategy as described in the paper to look for solutions for each instance of the problem. Using a heuristic to not only get closer to the formation of the acrostic, but also to better the overall quality of the texts generated.

\section{Acrostic Definition}
An acrostic is a specific type of text which encodes a certain word. In an acrostic if every starting letter of a certain amount of lines is taken, together they form a word.

\begin{figure}[H]
	\begin{quote}
 Knuth ist der Sohn eines Lehrers. Er besuchte die Milwaukee Luthera- \\
	n High School und begann sein Physikstudium am California Instit- \\
	ute of Technology im September 1956. Aus zweierlei Gründen schlug er \\
	tatsächlich seinem zweiten Studienjahr jedoch den Weg zur Mat-\\
	hematik ein: Zum einen löste er ein Problem eines seiner \\
	Mathematikprofessoren, was ihm eine 1,0 als Note einbrachte, zum \\
	anderen fand er wenig Gefallen an den physikalischen Praktika. Er \\
	erhielt einen Bachelor- und einen Master-Abschluss 1960 an der Case \\
	Western Reserve University. 

	\end{quote}
	\caption{Example of Acrostic}
\end{figure}

%----------------------------------------------------------------------------------------
%	Description of own work
%----------------------------------------------------------------------------------------
\chapter{Generating Acrostics via Paraphrasing and Heuristic Search}

\section{Problem Definition}
The problem we intend on solving in this work is, through the use of several operators, changing the text in ways such that an acrostic of a given word in German is formed for a certain text. For that we need to not only arrange the text so that it contains the desired acrostic but also take care not to exceed constraints necessary for the correctness of the text, such as a minimum length and a maximun length for the lines. This problem can then be formulated as a search problem, in which several new texts are created from the initial one by the use of semantic as well as orthographic operations.  Care should also be taken to ensure a certain level of quality to the results generated by the algorithm. It's then needed to disfavor texts that have their correctness too heavily damaged by these aforementioned operations.

\section{Technological Considerations}
[CITE JAVA AS PROG. LANG.; CITE EXTERNAL RESOURCES (NetSpeak API, Redis NoSQL server, Thesuarus, LaTeX Hyphenation library); EXPLAIN DEFINITION OF DATA STRUCTURES]

\section{Modeling as Search Problem}
In this section we show how to model ACROSTIC GENERATION as a search problem. Let $s$ be a text (for which we intend to generate an acrostic) and let $\mathcal{T}$ be the universe of all paraphrased versions of $s$. The idea is to represent the elements of $\mathcal{T}$ as states or equivalently as nodes of a tree, where the tree is constructed as follows:
\begin{itemize}
\item $s$ represents the root node

\item if $n\in\mathcal{T}$ is a node, then its successor nodes are given by all nodes, which
are produced by applying a paraphrasing operator $\phi$ to $n$.

\end{itemize}

A node $\gamma\in\mathcal{T}$ that already encodes the acrostic is called a goal node. The set of all goal nodes is denoted by $\Gamma$.\\
Solving an instance of ACROSTIC GENERATION under the so-called state-space representation then means to find a path from $s$ to some goal node $\gamma\in\Gamma$. According to the 
ACROSTIC GENERATION problem several question might arise, such as:

\begin{itemize}

\item  how to control the exploration of the search space?

\item which solution candidate should be chosen?

\item can we avoid that states are revisited?

\end{itemize}

To overcome these issues our strategy is to employ an $A^*$-algorithm. This algorithm performs a best-first search leading to a result with the best possible quality.  



\section{Cost Measure}

In order to apply the $A^*$-algorithm it is essential to define cost measures for the paraphrasing operators. These measures enable us to quantify the cost of each path and to define the heuristic. To define the cost measures, we first assign a constant value $q\in [0,1]$, named local quality, to each paraphrasing operation. The local quality values are on the one hand defined based on our experiences and on the other hand are inspired by the values given in \cite{Stein}. The local qualities are summarized in the following table:


\begin{center}

\begin{tabular}{|c|c|}\hline
   Operator           & Local quality $q$ \\ \hline
   LineBreak          & 0.9 \\ \hline
   Hyphenation        & 0.8 \\ \hline
   WordInsertionDeletion    & 0.7 \\ \hline
   Synonym            & 0.3 \\ \hline
   WrongHyphenation   & 0.4 \\ \hline
   WrongSpelling      & 0.7 \\ \hline
\end{tabular}

\end{center}

The cost of an operator $\phi$ is then defined as $1/q$ where $q$ denotes the local quality of $\phi$. In the following we will write $q(n_0,n_1)$ for the local quality of the operator, which transforms node $n_0$ into $n_1$.\\
In order to apply the $A^*$-algorithm we need to define a cost estimation heuristic $f(n)=g(n)+h(n)$. Here, the function $g$ calculates the true cost from the start node $s$ to
the node $n$, while $h$ underestimates the cost of the path from node $n$ to a goal node $\gamma\in\Gamma$. It is known that the $A^*$-algorithm finds an optimal (least cost) solution. If moreover, the function $h$ is monotonic, no node needs to be processed more than once (cf. \cite{AstarAlgorithm}). In this case the algorithm can be described as follows:\\

First, we insert the start node into a priority queue. (The lower $f(n)$ for a given node $n$, the higher its priority.) As long as the queue is not empty we dequeue the element $n$ with the highest priority. If this element already encodes the acrostic, we are done! Otherwise we put $n$ in the
visitedSet and generate its neighbors. For each neighbor of $n$ we check that it was not already visited. When this is true, and the neighbor node is not contained in the priority queue, we add it to the priority queue; if the neighbor node it contained in the priority queue, we update the f(n) value if necessary.\\

A more precise formulation of the $A^*$-algorithm is given in the following pseudocode:\\

\newpage
\begin{algorithm}
\caption{$A^*$-algorithm}\label{Astar}
\begin{algorithmic}[1]
\Function{$A^*$}{$start$, $acrostic$}
\State $stateQueue\gets \{start\}$\Comment{Priority queue of nodes preferring nodes with lower cost}
\State $visitedSet\gets \emptyset$\Comment{Hash Set of nodes already visited}
\While{$stateQueue$ is not empty}     
\State $current\gets stateQueue.pop()$\Comment{get first element and remove it}
\If{$current$ encodes the $acrostic$}
\State \Return $current$ 
\EndIf
\State add $current$ to $visitedSet$
\ForAll{$successor$ in successor\_nodes($current$)} 
		\If{$successor$ in $visitedSet$}
			 \State continue
		\EndIf		
		\State
		\State $\textrm{tentative\_g\_value} \gets g(current) + 1/q(current,successor)$		
		\State
		\If{$successor$ not in $stateQueue$ or $\mathrm{tentative\_g\_value} < g(successor)$}
			\State $g(successor)\gets \mathrm{tentative\_g\_value}$
			\State $f(successor)\gets g(successor) + h(successor)$
			\If{$successor$ not in $stateQueue$}
				\State add $successor$ to $stateQueue$
			\EndIf
		\EndIf
	\EndFor			
\EndWhile			 
\Return failure
\EndFunction
\end{algorithmic}
\end{algorithm}


After we have explained how the $A^*$-algorithm functions, we proceed with describing the semantics of the cost estimation function $f(n)=g(n) + h(n)$.\\
Here, the function $g$ accumulates the true cost for the partial acrostic via a concrete path $s=n_0,n_1,\dots,n_k=n$, while $h(n)$ gives an underestimation of the cost for the remaining part of the acrostic. The function $f$ can be stated as:
  
\begin{displaymath}
f(n)= \underbrace{\sum_{i=1}^k 1/q(n_{i-1},n_i)}_{g(n)} + 
\underbrace{log_K(E(\tau(n)))\cdot \frac{1}{q_{\mathrm{max}}}}_{h(n)},
\end{displaymath} 

where $\tau(n)$ denotes the remaining acrostic $y$ that is associated with $n\in\mathcal{T}$. (If $x^{\prime}$ is the part of the acrostic $x$ that has already been constructed, then the remaining acrostic $y$ is given by $x=x^{\prime}y$.)\\
The mapping $E$ is a measure for the difficulty or \emph{effort} for generating the acrostic $y$. It is defined by $E(y) = e(y_1)\cdots e(y_n)$, where $e(u)$, for a letter $u$, denotes the multiplicative inverse of the occurrence probability for $u$ as a first letter. The first letter probabilities can be found in \cite{FirstLetterFrequencies}.\\
The logarithm base $K$ serves for normalization purposes. We define $K$ as the multiplicative inverse of the occurrence probability of the least frequent first letter in the remaining acrostic $y=\tau(n)$. In our example, we have $E(y)=K^{l_1}\cdots K^{l_n}$ with $0<l_i\leq 1$, this yields $log_K(E(y))=\sum_{i=1}^n l_i \leq n =|y|$, where $|y|$ means the number of letters in $y$.\\
Finally, $q_{\mathrm{max}}$ is the maximum of the local operator qualities. \\


Our choice of $h(n)$ entails two properties:

\begin{enumerate}

\item[(1)] it underestimates the length of the remaining acrostic and therefore ensures the admissibility characteristic. Here, we assume that the remaining acrostic $y$ could be solved in the best case within a number of $n$ steps. In each step an operator with a cost of at least $1/q_{\mathrm{max}}$ is applied.


\item[(2)] it is monotonic, i.e. for each two nodes $n_1,n_2\in\mathcal{T}$, such that $n_2$ is a successor of $n_1$, we have $h(n_1)-h(n_2)\le 1/q(n_1,n_2)$.


\end{enumerate}







To show the monotonicity of $h$ we consider two cases:\\
1. If $h(n_1)-h(n_2)=0$ then the inequality $h(n_1)-h(n_2)\le 1/q(n_1,n_2)$ is obviously satisfied.\\
2. If $h(n_1)-h(n_2)>0$ then one letter more, say $u$, of the acrostic has been generated. We can therefore write $\tau(n_1)=u\tau(n_2)$. Let $K_1$ and $K_2$ be the logarithm bases for computing $h(n_1)$ and $h(n_2)$, respectively. Then we have $K_1\ge K_2$, which yields the inequality
\begin{align*}
h(n_1)-h(n_2) &= log_{K_1}(\tau(n_1))-log_{K_2}(\tau(n_2))\\
			  &= log_{K_1}(u\tau(n_2))-log_{K_2}(\tau(n_2))\\
			  & \le log_{K_1}(u\tau(n_2))-log_{K_1}(\tau(n_2))\\
			  &= (log_{K_1}(u)+log_{K_1}(\tau(n_2))-log_{K_1}(\tau(n_2))\\
			  &= log_{K_1}(u) \\
			  & \le 1 \le 1/q(n_1,n_2).
\end{align*}
Hence we have verified that $h$ is indeed monotonic.\\


\textbf{Remark:} From the implementational point of view it might be possible that one operator generates more than one letter of the acrostic at a time. The reason for this is that an adjustment of the whole text is conducted after each operation, which could lead to formal difficulties with both admissibility as well as monotonicity. As an easy way out we suggest to modify the operator cost by adding it one up for each additional letter the operator generates.\\







The above two properties guarantee that the $A^*$-algorithm finds an optimal (least cost) solution without processing any node twice (cf.\cite{AstarAlgorithm}).\\
But the guarantee of optimality during best-first search might not be the ultimate goal.
When running the algorithm we observed that often, say the first 2-3 letters of the acrostic were generated one after the other, but then the algorithm proceeds with a node in higher layers. The reason for this is that the heuristic $h$ underestimate the cost too rigorously. As a consequence, the best-first search degenerates to a breadth-first search. Hence, especially when computing power is a scarce resource one should better be off with a depth-preferring strategy.


\section{Operators}
Below are the explanations over the implemented operators. Most of these operators were selected basically because their probability of happenning in a successful solution according to \cite[p.~2025]{Stein}.

\subsection{Ensure Constraints}
An operation was created to ensure the texts created by the other operations are within the constraints imposed such as minimum and maximum line length. A text containing an acrostic can't be given as an answer if it's not valid. One benefit of having this operator is that other operators do not have to worry about being compliant to the constraints, leaving these worries to be solved by this one instead. Another benefit is the added variability which it provides, allowing texts which would be otherwise unusable, and it's potentially valuable changes useless, to be used.

The algorithm proceeds as follows:
\begin{enumerate}
\item From top to bottom, look if each line is over the minimum length and under the maximum length.
\item If not then shift characters to and from the line below it until it is under the restrictions again.
\item Repeat steps one and two until the last line is reached.
\item For the last line, if it is too big, a new line has to be added while the last line is too big
\end{enumerate}

\begin{figure}[H]
	\begin{quote}
		\begin{alltt}
Bemerkungen zur modernen Darstellung nationaler Geschichte.
Eine neuere Darstellung deutscher Geschichte auf 335 Se-
iten kommt einer Nachfrage entgegen, nicht nur für den deutschen Bereich,
sondern offenbar auch im Ausland, wie die bald erfolgte Übersetz-
ung des hier angezeigten Bandes ins Englische ausweist.
		\end{alltt}
	\end{quote}
	\caption{Example of Ensure Constraints $-$ Original Text}
\end{figure}

\begin{figure}[H]
	\begin{quote}
		\begin{alltt}
			Bemerkungen zur modernen Darstellung nationaler Geschichte.
			Eine neuere Darstellung deutscher Geschichte auf 335 Se-
			iten kommt einer Nachfrage entgegen, nicht nur für den deutschen Bere-
			ich, sondern offenbar auch im Ausland, wie die bald erfolgte Übersetz-
			ung des hier angezeigten Bandes ins Englische ausweist.
		\end{alltt}
	\end{quote}
	\caption{Example of Ensure Constraints $-$ Altered Text}
\end{figure}

\subsection{Wrong Hyphenation}
This operator works by hyphenating words in the text without care for hyphenation rules. The idea behind it is that some letters would be too hard or simply impossible to obtain without corrupting the text somewhat.
For this reason, Wrong Hyphenation has a very low quality measure.

The algorithm goes as follows:
\begin{enumerate}
\item For each line of a given text the last word is taken.
\item If this word is already hyphenated, nothing is done.
\item Otherwise, every permutation of possible hyphenations is created.
\item A new text is created from each of these permutations.
\end{enumerate}

\begin{figure}[H]
	\begin{quote}
		\begin{alltt}
			Jo fuhr langsam auf den Patio. Wagen neben Wagen parkte dort. 
			Aus der Bar drang Susans rauchige Altstimme. Ein junges Paar stieg in einen
			Camero. Jo wartete, bis sie abfuhren. Er rollte in die Lücke, stellte
			den Motor ab und schaltete die Scheinwerfer aus.
		\end{alltt}
	\end{quote}
	\caption{Example of Wrong Hyphenation $-$ Original Text}
\end{figure}

\begin{figure}[H]
	\begin{quote}
		\begin{alltt}
			Jo fuhr langsam auf den Patio. Wagen neben Wagen parkte d-
			ort. Aus der Bar drang Susans rauchige Altstimme. Ein junges Paar sti-
			eg in einen Camero. Jo wartete, bis sie abfuhren. Er rollte in die Lü-
			cke, stellte den Motor ab und schaltete die Scheinwerfer aus.
		\end{alltt}
	\end{quote}
	\caption{Example of Wrong Hyphenation $-$ Paraphrased Text}
\end{figure}

\subsection{Wrong Spelling}
Wrong Spelling in this project utilizes several common misspellings in the german language. With a low quality, this operator changes letters with an umlaut such as "ä", "ö" and "ü", and turns them into "ae", "oe", and "ue" as well as changing "ß" to "ss". This way some variety can be added to the text when absolutely needed to generate an acrostic that would be otherwise impossible to create.
These steps are taken to create new texts:
\begin{enumerate}
\item Look in every line for instances of the cases mentioned above.
\item When one of these is found, generate a new text.
\item Continue looking for other instances that can be changed.
\end{enumerate}
With this operation, some acrostics which would be otherwise impossible to create, can be generated. It does however add a great deal of incorrectness to the text, it is then given a very low quality value and is therefore a pretty uncommon operator.

\begin{figure}[H]
	\begin{quote}
		\begin{alltt}
			Die etymologischen Vorformen von "deutsch" bedeuteten ursprünglich 
			"zum Volk gehörig", wobei das Adjektiv zunächst die Dialekte des 
			kontinental-westgermanischen Dialektkontinuums bezeichnete. Die 
			Bezeichnung Deutschland wird seit dem 15. Jahrhundert verwendet, ist
			in einzelnen Schriftstücken aber schon früher bezeugt.
		\end{alltt}
	\end{quote}
	\caption{Example of Wrong Spelling $-$ Original Text}
\end{figure}

\begin{figure}[H]
	\begin{quote}
		\begin{alltt}
			Die etymologischen Vorformen von "deutsch" bedeuteten urspru- 
			englich "zum Volk gehörig", wobei das Adjektiv zunächst die Dialekte de- 
			s kontinental-westgermanischen Dialektkontinuums bezeichnete. Die 
			Bezeichnung Deutschland wird seit dem 15. Jahrhundert verwendet, ist 
			in einzelnen Schriftstücken aber schon früher bezeugt. 
		\end{alltt}
	\end{quote}
	\caption{Example of Wrong Spelling $-$ Paraphrased Text}
\end{figure}

\subsection{Word Insertion or Deletion}
The idea around this operator is to insert words in the text or delete words from it, in order to insert new letters and accomplish the goal acrostic or to remove words and change the position of words inside the text. \par

To illustrate the execution, consider the following text\footnote{This text was adapted for didactic purposes from \url{http://cornelia.siteware.ch/blog/wordpress/2008/11/03/sich-vorstellen-horverstehen}. Access in January, 2015}:

\begin{figure}[H]
\begin{quote}
\begin{alltt}
Ah ja, ich heisse Frederik Hoske und ich bin 13 Jahre. \textit{Ich kann nicht
vorstellen}, weil ich kaum Deutsch sprechen kann. Trotzdem versuche ich
es. Ich habe zwei Geschwister Mein Bruder der 16 Jahre alt ist und
meine Schwester ist elf.
\end{alltt}
\end{quote}
\caption{Example of word insertion application $-$ Original Text}
\end{figure}

After inserting the word \emph{"mir"} in the sentence \emph{"Ich kann nicht vorstellen"} in the first line and after breaking a line right before \emph{"Trotzdem"} the algorithm can reach the acrostic \emph{amt}. Note that the insertion of \emph{"mir"} was crucial for the result, once that the letter \emph{m} was not there.

\begin{figure}[H]
\begin{quote}
\begin{alltt}
\textcolor{Blue}{A}h ja, ich heisse Frederik Hoske und ich bin 13 Jahre. \textit{Ich kann
\textcolor{Blue}{m}ir nicht vorstellen}, weil ich kaum Deutsch sprechen kann.
\textcolor{Blue}{T}rotzdem versuche ich es. Ich habe zwei Geschwister Mein Bruder der
16 Jahre alt ist und meine Schwester ist elf.
\end{alltt}
\end{quote}
\caption{Example of word insertion application $-$ Paraphrased Text}
\end{figure}

The Word Insertion or Deletion operator takes as input a text. Then first it tries to insert a new word in each space and second tries to remove each word of the text. The condition to insert a new word \emph{w} in the \emph{i-th} space of the text is that \emph{w} has to fit the context around the \emph{i-th} space. It means that from the set of all possible words of the language, only a restricted subset can be inserted in this place. More specifically, the algorithm starts by taking for each space in the text \emph{n} words around it as context $-$ In our implementation in this context $n = 4$. This is a so called n-gram, an array of words. After this, the n-gram just taken is sent to the context database (which is in this implementation the Netspeak API \cite{Netspeak}), that returns the possible words that could be inserted in the required space. For each of these possible words a new version of the text is created with the word inside. \par

Analogously, for each word \emph{w} in the text a n-gram including the words around it is created $-$ In our implementation we take two words from each side, so here $n=5$. \emph{w} is then taken out of the n-gram, which is tested against the context database to check whether this n-gram is frequent enough in the language. In other words, the frequency \emph{F} of such n-gram is compared with a minimum acceptable frequency $F_{min}$. If $F \ge F_{min}$ a new version of the text without \emph{w} is created. Our implementation allows the adjustment of $F_{min}$, but we set it to zero as default, so a broader set of deletions is executed. \par

The queries to the context database are made in form of HTTP requests to the Netspeak web service using the Netspeak API. \par

\subsection{Synonyms}
The synonym operator has the goal of changing words in the text for other words, which have similar meaning. In general the operator takes a text as input and generates a set of new texts, in which each text has a word replaced by a synonym.

In order to perform the replacements it is required a synonym dictionary, which is know as thesaurus. In our implementation we used Open Thesaurus \cite{OpenThesuarus}, which is available for download for free. This data source is available as a plain text file, but as the dictionary is accessed many times during the execution of the algorithm, it soon becomes intractable to handle a text file as a database.

To solve this problem we decided to use a NoSQL database server \cite{NoSQL}, namely, Redis. Redis is an open source advanced key-value pair cache and store \cite{Redis}. Into the database server we load once the data from the thesaurus in a structured way where, every word is added as a key that points to a set of synonyms. Thus the application can easily and efficiently find similar terms for a given word only by accessing this key.

Naturally it is then required that the Redis server is running and listening to requests when the application runs, and that it has been once loaded by our script with the data from the dictionary.

\begin{figure}[H]
\begin{quote}
\begin{alltt}
Diese war unterdessen in das Aussehen gerückt, was die Gegenwart
\textbf{damals} hatte, sie war sehr schön, aber immer traurig, weil sie sich
vor ihrem Bräutigam fürchtete und weil sie von den Schwestern, die
keinen Mann bekommen, beständig seinetwegen geneckt wurde. Eines Tages
rief ein heller Trompetenschall alle drei Schwestern ans Fenster.
\end{alltt}
\end{quote}
\caption{Example of synonym application $-$ Original Text}
\end{figure}

\begin{figure}[H]
\begin{quote}
\begin{alltt}
Diese war unterdessen in das Aussehen gerückt, was die Gegenwart
\textbf{in jenen tagen} hatte, sie war sehr schön, aber immer traurig, weil si-
e sich vor ihrem Bräutigam fürchtete und weil sie von den Schwe-
stern, die keinen Mann bekommen, beständig seinetwegen geneckt wurde.
Eines Tages rief ein heller Trompetenschall alle drei Schwestern ans
Fenster.
\end{alltt}
\end{quote}
\caption{Example of synonym application $-$ Paraphrased Text}
\end{figure}

The application of the synonym operation brings much more possibilities for new paraphrased versions of the original text. It happens because, when comparing with Word Insertion and Deletion, the probability of changing a word in the text with this operation is higher since it does not check the context around the changed word, therefore allowing many words to be replaced.

Consequently, the drawback is a considerable loss of quality in the results, because of the fact that synonyms are strongly related to context, and some replacements may change substantially the meanings of the resulting texts.

\subsection{Line break}
The fastest and most basic operation is the line break. A line break can be applied in two cases:

\begin{itemize}
	\item After a word when the line length lies in the $[l_{min},l_{max}]$-window,
	given by the line length constraints.
	\item After the end of a sentence.
\end{itemize}
After performing a line break, the lines following the line break have
to be aligned again to satisfy the line length constraints.

For this task we apply a greedy word wrap algorithm, which works as follows: we split the text into words,
put the words on the line as long as there is free space, if there is no free space left, we continue with the next line.

When applying the greedy word wrap algorithm we have to ensure that
there is no word of length $> 20$ in the initial text. Otherwise it might happen, that the minimal line
length constraint is not fulfilled.

Identifying the end of a sentence in general is a difficult problem. One reason for this is that a period
might occur in several contexts, e.g.

\begin{itemize}

	\item abbreviations (Prof., Dr., d.h., z.B., ...)
	\item ordinal numbers (der 26. April, Joseph II., 2. Auflage, ...)
	\item numbers (10.1312, 192.11301, ...)

\end{itemize}

Another issue is that there is a wide variety of punctuations which could mark the end of a sentence. These punctuations include question marks, exclamation marks, ellipses, semi-cola, cola.

To overcome these problems we make use of a sentence-splitter library, called Sentrick (cf. \cite{Sentrick}).

\subsection{Hyphenation}
Related to the line break are hyphenations. A hyphenation is applicable if the line after hyphenating (and line breaking) has a length of at least $l_{min}=50$.
For hyphenating a word we employ a re-implementation of Knuth's hyphenation algorithm in TEX (cf. \cite{Hyphenation}).

After the hyphenation, the text following the hyphen has to be aligned again to satisfy the line length constraints.

Analog to the line break operation, in order to rearrange the lines we apply a greedy word wrap algorithm.

%----------------------------------------------------------------------------------------
%	Evaluation of the Results
%----------------------------------------------------------------------------------------
\chapter{Evaluation of the Results}
The goal of the evaluation of the developed application is to show that it is able to generate results of acrostics for texts written in the german language. So we analyze the success rate, the operation application rate and the influence of the several variations in the test cases. For that we set up a test suit of 100 texts, and run the application combining two different configurations: The search algorithm, which can be A* ($f(x) = g(x) + h(x)$) or greedy search A* ($f(x) = h(x)$); and the chosen acrostic, which can then be self-referential or the most common word that starts with the first letter of the text. This list of words was obtained from Wortschatz Universität Leipzig\footnote{\url{http://wortschatz.uni-leipzig.de/html/wliste.html}}.

The configuration on the search algorithm allows the program to be switched between concerning, or not, about the quality of the applied operations. That means, when running it in the greedy search mode, all operations have a cost of zero. That leads to a goal oriented computation, where the choice of the operation does not influence the choice of the solution path in the search space. In the other mode, it is possible to have an optimization oriented computation, which is achieved by running with the regular A*-algorithm \cite{AStar}.

In order to vary the input domain, we ran test cases in two different possibilities: the so called, self-referential, where the acrostic is the first word in the input text; and the most common word, in which the acrostic is the word that has the highest occurrence probability and starts with the first letter of the input text. It should be noted that, in every test case the first letter of the text and of the acrostic are the same, this is a crucial decision for the general success of the application, as pointed out in \cite{Stein}. In this work, which served as inspiration for this algorithm, should that condition not be held, only about 20\% of the test cases could generate results.

\section{Experiment Setup}
First of all we selected a set of input examples for the application. For that two data sources were used: Limas-Korpus\footnote{\url{http://www.korpora.org/Limas/}} and the public domain books repository of the Gutenberg Project\footnote{\url{http://www.gutenberg.org/browse/languages/de}}. On the whole we ran the tests with a base of  100 test cases. As we have two possible acrostics for each text we were able to run a total of 200 different executions.

We let the Redis database started in the same system where the tests run. The initialization of data into the key-value store server is realized once by the script that processes the thesaurus and it took no more than a couple of minutes to be successfully completed. For the employment of the NetSpeak API web service an internet connection is required. As advised by \cite{Stein}, we set line length constraints of $l_{min} = 50$ and $l_{max} = 70$ because of the flexibility it brings to operators like line break and hyphenation. As the application may run for a long time, we set a timeout for its execution. For that we decided on a base of 15 minutes, which can eventually be increased or decreased according to the availability of resources and time of the user.

\section{Experiment Discussion}

\begin{table}
\centering
\begin{tabular}{l | l | l | l | l}
	\hline
	\textbf{Configuration} & \textbf{Success Rate} & \textbf{Runtime} & \textbf{Nodes} & \textbf{Timeout Rate} \\ \hline
		\textbf{A* + Self-reference}	& S	& S	& S	& S \\ \hline
		\textbf{A* + Most Common}		& S	& S	& S	& S \\ \hline
		\textbf{Greedy A* + Self-reference}	& S & S	& S	& S \\ \hline
		\textbf{Greedy A* + Most Common}	& S & S	& S	& S \\
	\hline
\end{tabular}
\label{tab:results}
\caption{Experimental results}
\end{table}


%----------------------------------------------------------------------------------------
%	Summary of Findings
%----------------------------------------------------------------------------------------
\chapter{Summary of Findings}
The algorithm described in this report is able to build acrostics with short words (we have achieved acrostics with 6 letters) in reasonable time. These acrostics have regular quality, in the sense that some paraphrasing operations are easily perceived by the reader $-$ the wrong hyphenation is one of them. The use of synonyms without checking the context lowers the quality as well. Additionally, the algorithm can generate a result in a few minutes for a good number of cases, what makes its use possible in some practical situation. The review of the result by a human is however necessary. \par

In despite of these results, we did not develop all the operators described in the reference paper (\cite{Stein}) mainly for project time reasons. A broader experimental discussion with a higher number of tests and a discussion about quality metrics on the text would be interesting as well, although it would be out of the scope of the project. \par

What should be noted in the problem of generating acrostics, is that the success depends highly on certain conditions of the text. When the target acrostic has uncommon letters, it becomes naturally more difficult to accomplish the result. This yields the need for more powerful operators, operators able to change more the structure of the text or insert new elements that aids the generation of the desired letters in the beginning of the lines. In this vein, a grammatical operator that could swap the position of grammatical elements would be helpful, like illustrated in the example below. \par

\begin{figure}[H]
\begin{quote}
\begin{alltt}
Wenn Sie wissen, was Sie wollen, wird es einfacher.
Wenn Sie, was Sie wollen, wissen, wird es einfacher.
Es wird einfacher, wenn Sie wissen, was Sie wollen.
Einfacher wird es, wenn Sie wissen, was Sie wollen.
\end{alltt}
\end{quote}
\caption{Example of possible additional operator}
\end{figure}

Another possible improvement is the development of a non-empirical method for defining costs for each operation. The choice of such parameters seems to be vital for the success of the search method. According to \cite[p.~2023]{Stein}, a bad choice may lead to a breadth-first search, while if the computing resources are scarce the desired is a depth-first search. The time consumed by requests to the n-gram web service are also sources of problems for such environments. But in this case the implementation of a n-gram database locally may smooth the problem. \par

The use of the Netspeak API seems to be another issue, since it does not produce many options for inserting or removing words. A solution would be the implementation of the complete Google n-gram database locally. Even though the use of context in form of n-gram are sometimes too restrictive. This could be therefore a discussion for future work.


%----------------------------------------------------------------------------------------
%	References
%----------------------------------------------------------------------------------------
\begin{thebibliography}{1}
\bibitem{Stein}
	Benno Stein, Matthias Hagen, and Christof Bräutigam. \emph{Generating Acrostics via Paraphrasing and Heuristic Search}. \\
	In Junichi Tsujii and Jan Hajic, editors, 25th International Conference on Computational Linguistics (COLING 14), pages 2018-2029, August 2014. Association for Computational Linguistics.
\bibitem{Netspeak}
	Martin Potthast, Martin Trenkmann, and Benno Stein.
	\emph{Netspeak: Assisting Writers in Choosing Words}. \\
	In Cathal Gurrin et al, editors, Advances in Information Retrieval. 32nd European Conference on Information Retrieval (ECIR 10) volume 5993 of Lecture Notes in Computer Science, pages 672, Berlin Heidelberg New York, March 2010. Springer.
\bibitem{OpenThesuarus}
	\emph{Synonyme - OpenThesaurus - Deutscher Thesaurus}.
	Available on: $<$\url{https://www.openthesaurus.de}$>$.
	Accessed in: January 2015.
\bibitem{NoSQL}
	Jing Han; Haihong, E.; Guan Le; Jian Du. \emph{Survey on NoSQL database}. Pervasive Computing and Applications (ICPCA), 2011 6th International Conference on, pp.363,366, 26-28 October 2011.
\bibitem{Redis}
	\emph{Redis}.
	Available on: $<$\url{http://redis.io}$>$.
	Accessed in: January 2015.
\bibitem{Sentrick}
\emph{Sentrick}.
Available on: \url{http://sourceforge.net/projects/sentrick/}
Accessed in: January 2015.


\bibitem{Hyphenation}
\emph{TexHyphJ}.
Availabe on: \url{http://sourceforge.net/projects/texhyphj/}
Accessed in: January 2015.

\bibitem{FirstLetterFrequencies}
\emph{Private Communication} with Rainer Perkuhn,
Institut für Deutsche Sprache Programmbereich Korpuslinguistik, via Email, in Dezember 2014. He send us two files of an old study of Cyril Belica, including the first letter frequencies.  

\bibitem{AstarAlgorithm}
\url{http://de.wikipedia.org/wiki/A*-Algorithmus}














\bibitem{AStar}
	Quasthoff, U.; M. Richter; C. Biemann. \emph{Corpus Portal for Search in Monolingual Corpora}. Proceedings of the fifth international conference on Language Resources and Evaluation, LREC 2006, Genoa, pp. 1799-1802.
\bibitem{AStar}
	Dechter, Rina; Pearl, Judea. \emph{Generalized best-first search strategies and the optimality of A*}. Journal of the ACM (JACM), Volume 32 Issue 3, Pages 505-536, New York, July 1985.
\end{thebibliography}

%----------------------------------------------------------------------------------------
%	Appendix
%----------------------------------------------------------------------------------------
%\appendix
%\chapter{Appendix}


\end{document}
